#!/usr/bin/env python3
"""
Petz Recovery Network: Learning the MaxEnt Recovery Map

This script trains a neural network to learn the Petz Recovery Map for SGC dynamics.
It demonstrates that an external ML agent can correct "drift" that the generator
cannot correct internally (due to the Coherence Obstruction theorem).

Key Concepts:
- The Petz map is the MaxEnt recovery: recovers state with fewest assumptions
- Minimizing Variational Free Energy = Maximizing entropy of recovery distribution
- This is the reverse step in Diffusion Models

Connection to Lean Formalization:
- src/SGC/Bridge/Recovery.lean: Defines PetzRecoveryMap and proves properties
- src/SGC/Bridge/CoherenceObstruction.lean: Proves internal correction impossible

Usage:
    python train_petz_net.py [--wells 3] [--dim 50] [--epochs 1000]
    streamlit run train_petz_net.py

Author: SGC Project
"""

import numpy as np
from scipy.linalg import expm
from dataclasses import dataclass
from typing import Tuple, List, Optional
import argparse

# Check for optional dependencies
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("PyTorch not installed. Install with: pip install torch")

try:
    import streamlit as st
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SGC DYNAMICS ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SGCSystem:
    """A metastable system with multiple wells for SGC dynamics."""
    n_states: int
    n_wells: int
    generator: np.ndarray  # Rate matrix L
    stationary: np.ndarray  # Stationary distribution Ï€
    partition: np.ndarray   # Block assignment for each state
    
    @classmethod
    def create_multiwell(cls, n_states: int = 50, n_wells: int = 3, 
                         intra_rate: float = 1.0, inter_rate: float = 0.1) -> 'SGCSystem':
        """Create a metastable system with multiple potential wells.
        
        Args:
            n_states: Total number of microstates
            n_wells: Number of metastable wells (blocks)
            intra_rate: Rate of transitions within wells (fast)
            inter_rate: Rate of transitions between wells (slow)
        """
        states_per_well = n_states // n_wells
        partition = np.repeat(np.arange(n_wells), states_per_well)
        
        # Pad if n_states not divisible by n_wells
        if len(partition) < n_states:
            partition = np.concatenate([partition, 
                np.full(n_states - len(partition), n_wells - 1)])
        
        # Build generator matrix
        L = np.zeros((n_states, n_states))
        for i in range(n_states):
            for j in range(n_states):
                if i != j:
                    if partition[i] == partition[j]:
                        # Intra-well transition (fast)
                        L[i, j] = intra_rate
                    else:
                        # Inter-well transition (slow)
                        L[i, j] = inter_rate
            # Diagonal: ensure rows sum to 0
            L[i, i] = -np.sum(L[i, :])
        
        # Compute stationary distribution (uniform for this symmetric case)
        stationary = np.ones(n_states) / n_states
        
        return cls(n_states, n_wells, L, stationary, partition)
    
    def evolve(self, p: np.ndarray, t: float) -> np.ndarray:
        """Evolve distribution p for time t using heat kernel e^{tL}."""
        P_t = expm(t * self.generator)
        return P_t @ p
    
    def sample_trajectory(self, p0: np.ndarray, dt: float, n_steps: int) -> np.ndarray:
        """Generate trajectory of distributions."""
        trajectory = np.zeros((n_steps + 1, self.n_states))
        trajectory[0] = p0
        for i in range(n_steps):
            trajectory[i + 1] = self.evolve(trajectory[i], dt)
        return trajectory


def relative_entropy(p: np.ndarray, q: np.ndarray, eps: float = 1e-10) -> float:
    """Compute KL divergence D(p||q) = Î£ p log(p/q)."""
    p_safe = np.clip(p, eps, 1.0)
    q_safe = np.clip(q, eps, 1.0)
    return np.sum(p_safe * np.log(p_safe / q_safe))


def classical_fidelity(p: np.ndarray, q: np.ndarray) -> float:
    """Compute classical fidelity F(p,q) = (Î£ âˆš(pÂ·q))Â²."""
    return np.sum(np.sqrt(np.abs(p * q))) ** 2


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PETZ RECOVERY NETWORK (PyTorch)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if TORCH_AVAILABLE:
    class PetzRecoveryNet(nn.Module):
        """Neural network to learn the Petz recovery map.
        
        Architecture: MLP that maps current state â†’ previous state
        Loss: Variational Free Energy (equivalent to MaxEnt recovery)
        """
        
        def __init__(self, state_dim: int, hidden_dim: int = 128):
            super().__init__()
            self.network = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.BatchNorm1d(hidden_dim),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, state_dim),
                nn.Softmax(dim=-1)  # Output is a probability distribution
            )
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            return self.network(x)
    
    
    def variational_free_energy(p_pred: torch.Tensor, p_target: torch.Tensor, 
                                 pi: torch.Tensor, beta: float = 1.0) -> torch.Tensor:
        """Variational Free Energy loss = -ELBO.
        
        F = E_q[log q(x)] - E_q[log p(x|y)] + Î²Â·D(q||Ï€)
        
        Minimizing F is equivalent to:
        1. Maximizing log-likelihood of recovery
        2. Maximizing entropy of the recovery distribution (MaxEnt principle)
        
        This is exactly the Petz map objective!
        """
        eps = 1e-10
        
        # Reconstruction loss: -E[log p(target|pred)]
        # Approximated as cross-entropy
        reconstruction = -torch.sum(p_target * torch.log(p_pred + eps), dim=-1)
        
        # KL regularization toward stationary distribution
        kl_regularization = torch.sum(p_pred * torch.log((p_pred + eps) / (pi + eps)), dim=-1)
        
        return torch.mean(reconstruction + beta * kl_regularization)
    
    
    def train_petz_network(system: SGCSystem, 
                           n_trajectories: int = 100,
                           trajectory_length: int = 20,
                           dt: float = 0.1,
                           epochs: int = 500,
                           lr: float = 1e-3,
                           hidden_dim: int = 128,
                           verbose: bool = True) -> Tuple['PetzRecoveryNet', List[float]]:
        """Train the Petz recovery network on SGC trajectories.
        
        Args:
            system: SGC metastable system
            n_trajectories: Number of training trajectories
            trajectory_length: Steps per trajectory
            dt: Time step
            epochs: Training epochs
            lr: Learning rate
            hidden_dim: Hidden layer dimension
            verbose: Print progress
            
        Returns:
            Trained network and loss history
        """
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Generate training data
        # Data: (current_state, previous_state) pairs
        X_train = []  # Current states (after evolution)
        Y_train = []  # Previous states (before evolution)
        
        for _ in range(n_trajectories):
            # Random initial distribution (concentrated on random state)
            init_state = np.random.randint(system.n_states)
            p0 = np.zeros(system.n_states)
            p0[init_state] = 1.0
            
            # Add small noise to avoid pure delta distributions
            p0 = 0.9 * p0 + 0.1 * system.stationary
            
            # Generate trajectory
            trajectory = system.sample_trajectory(p0, dt, trajectory_length)
            
            # Create training pairs: (p_{t+1}, p_t)
            for t in range(trajectory_length):
                X_train.append(trajectory[t + 1])  # Current (after forward step)
                Y_train.append(trajectory[t])      # Target (before forward step)
        
        X_train = torch.FloatTensor(np.array(X_train)).to(device)
        Y_train = torch.FloatTensor(np.array(Y_train)).to(device)
        pi_tensor = torch.FloatTensor(system.stationary).to(device)
        
        # Initialize network
        model = PetzRecoveryNet(system.n_states, hidden_dim).to(device)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.5)
        
        # Training loop
        loss_history = []
        for epoch in range(epochs):
            model.train()
            optimizer.zero_grad()
            
            # Forward pass
            p_recovered = model(X_train)
            
            # Compute loss (Variational Free Energy)
            loss = variational_free_energy(p_recovered, Y_train, pi_tensor)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            scheduler.step(loss)
            
            loss_history.append(loss.item())
            
            if verbose and (epoch + 1) % 100 == 0:
                print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.6f}")
        
        return model, loss_history
    
    
    def evaluate_recovery(model: 'PetzRecoveryNet', system: SGCSystem, 
                          dt: float = 0.1, n_test: int = 100) -> dict:
        """Evaluate recovery quality of trained network.
        
        Returns metrics comparing to:
        1. Perfect recovery (ground truth)
        2. Trajectory closure bound O(ÎµÂ·t)
        """
        device = next(model.parameters()).device
        model.eval()
        
        fidelities = []
        kl_divergences = []
        
        with torch.no_grad():
            for _ in range(n_test):
                # Random initial state
                init_state = np.random.randint(system.n_states)
                p0 = np.zeros(system.n_states)
                p0[init_state] = 1.0
                p0 = 0.9 * p0 + 0.1 * system.stationary
                
                # Forward evolution
                p1 = system.evolve(p0, dt)
                
                # Network recovery
                p1_tensor = torch.FloatTensor(p1).unsqueeze(0).to(device)
                p_recovered = model(p1_tensor).cpu().numpy().squeeze()
                
                # Metrics
                fidelities.append(classical_fidelity(p_recovered, p0))
                kl_divergences.append(relative_entropy(p0, p_recovered))
        
        return {
            'mean_fidelity': np.mean(fidelities),
            'std_fidelity': np.std(fidelities),
            'mean_kl': np.mean(kl_divergences),
            'std_kl': np.std(kl_divergences),
            'fidelities': fidelities,
            'kl_divergences': kl_divergences
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANALYTICAL PETZ MAP (for comparison)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def analytical_petz_map(system: SGCSystem, t: float) -> np.ndarray:
    """Compute the analytical Petz recovery map.
    
    For the forward channel M = e^{tL}, the Petz map is:
    R_Ï€ = Î ^{1/2} M^â€  Î ^{-1/2} (Â·) Î ^{-1/2} M^â€  Î ^{1/2}
    
    In the classical (diagonal) case, this simplifies to Bayesian inversion.
    """
    M = expm(t * system.generator)  # Forward channel
    pi = system.stationary
    
    # Petz map for classical channels: R[i,j] = M[j,i] * Ï€[i] / (Î£_k M[k,i] Ï€[k])
    R = np.zeros_like(M)
    for i in range(system.n_states):
        for j in range(system.n_states):
            # Bayesian inversion: P(i|j) = P(j|i) P(i) / P(j)
            # P(j) = Î£_i P(j|i) P(i)
            p_j = np.sum(M[:, j] * pi)
            if p_j > 1e-10:
                R[i, j] = M[j, i] * pi[i] / p_j
    
    return R


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STREAMLIT UI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_streamlit_app():
    """Interactive Streamlit application for Petz Recovery visualization."""
    
    st.set_page_config(
        page_title="Petz Recovery Network",
        page_icon="ğŸ”„",
        layout="wide"
    )
    
    st.markdown("""
    <style>
        .header-petz {
            background: linear-gradient(90deg, #ff6b6b, #4ecdc4);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-size: 2.5rem;
            font-weight: bold;
            text-align: center;
        }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<p class="header-petz">ğŸ”„ Petz Recovery Network</p>', unsafe_allow_html=True)
    
    st.markdown("""
    **The MaxEnt Solution to the Coherence Obstruction**
    
    This demo shows how a neural network learns the Petz Recovery Mapâ€”the canonical 
    way to "undo" forward dynamics. The Coherence Obstruction theorem proves this 
    cannot happen *internally* (the generator L cannot self-correct), but an 
    *external* ML agent can learn the recovery.
    
    > *"The neural network implements measurement-feedback control, paying Landauer's cost."*
    """)
    
    # Sidebar controls
    st.sidebar.header("System Configuration")
    n_wells = st.sidebar.slider("Number of Wells", 2, 5, 3)
    n_states = st.sidebar.slider("States per Well", 10, 30, 20) * n_wells
    inter_rate = st.sidebar.slider("Inter-well Rate (Îµ)", 0.01, 0.5, 0.1)
    
    st.sidebar.header("Training Configuration")
    epochs = st.sidebar.slider("Training Epochs", 100, 2000, 500)
    n_trajectories = st.sidebar.slider("Training Trajectories", 50, 500, 100)
    dt = st.sidebar.slider("Time Step (Î”t)", 0.05, 0.5, 0.1)
    
    # Create system
    system = SGCSystem.create_multiwell(n_states, n_wells, intra_rate=1.0, inter_rate=inter_rate)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“Š System Structure")
        
        # Visualize generator matrix
        fig_gen = go.Figure(data=go.Heatmap(
            z=np.log10(np.abs(system.generator) + 1e-10),
            colorscale='RdBu',
            zmid=0
        ))
        fig_gen.update_layout(
            title="Generator Matrix L (log scale)",
            xaxis_title="To State",
            yaxis_title="From State",
            height=400
        )
        st.plotly_chart(fig_gen, use_container_width=True)
        
        st.markdown(f"""
        **System Parameters:**
        - States: {n_states}
        - Wells: {n_wells}
        - Inter-well rate Îµ: {inter_rate}
        - Metastability ratio: {1.0 / inter_rate:.1f}x
        """)
    
    with col2:
        st.subheader("ğŸ”¬ Partition Structure")
        
        # Visualize partition
        block_sizes = [np.sum(system.partition == k) for k in range(n_wells)]
        fig_part = go.Figure(data=go.Bar(
            x=[f"Well {k}" for k in range(n_wells)],
            y=block_sizes,
            marker_color=[f'hsl({k * 360 // n_wells}, 70%, 50%)' for k in range(n_wells)]
        ))
        fig_part.update_layout(
            title="States per Well (Coarse-Grained Blocks)",
            yaxis_title="Number of States",
            height=400
        )
        st.plotly_chart(fig_part, use_container_width=True)
    
    # Training section
    st.markdown("---")
    st.subheader("ğŸ§  Train Petz Recovery Network")
    
    if not TORCH_AVAILABLE:
        st.error("PyTorch not installed. Install with: `pip install torch`")
        return
    
    if st.button("ğŸš€ Train Network", type="primary"):
        with st.spinner("Training Petz Recovery Network..."):
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Train with progress updates
            model, loss_history = train_petz_network(
                system, 
                n_trajectories=n_trajectories,
                trajectory_length=20,
                dt=dt,
                epochs=epochs,
                verbose=False
            )
            
            progress_bar.progress(100)
            status_text.text("Training complete!")
            
            # Store in session state
            st.session_state['model'] = model
            st.session_state['loss_history'] = loss_history
            st.session_state['system'] = system
            st.session_state['dt'] = dt
    
    # Results section
    if 'model' in st.session_state:
        st.markdown("---")
        st.subheader("ğŸ“ˆ Training Results")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Loss curve
            fig_loss = go.Figure()
            fig_loss.add_trace(go.Scatter(
                y=st.session_state['loss_history'],
                mode='lines',
                name='Variational Free Energy',
                line=dict(color='#4ecdc4')
            ))
            fig_loss.update_layout(
                title="Training Loss (Variational Free Energy)",
                xaxis_title="Epoch",
                yaxis_title="Loss",
                yaxis_type="log",
                height=400
            )
            st.plotly_chart(fig_loss, use_container_width=True)
        
        with col2:
            # Evaluate recovery
            metrics = evaluate_recovery(
                st.session_state['model'], 
                st.session_state['system'],
                dt=st.session_state['dt']
            )
            
            # Fidelity histogram
            fig_fid = go.Figure()
            fig_fid.add_trace(go.Histogram(
                x=metrics['fidelities'],
                nbinsx=30,
                marker_color='#ff6b6b',
                name='Recovery Fidelity'
            ))
            fig_fid.add_vline(x=1.0, line_dash="dash", line_color="green",
                             annotation_text="Perfect Recovery")
            fig_fid.update_layout(
                title=f"Recovery Fidelity (Mean: {metrics['mean_fidelity']:.4f})",
                xaxis_title="Fidelity F(p_recovered, p_original)",
                yaxis_title="Count",
                height=400
            )
            st.plotly_chart(fig_fid, use_container_width=True)
        
        # Key metrics
        st.markdown("### ğŸ“Š Recovery Quality Metrics")
        
        metric_cols = st.columns(4)
        metric_cols[0].metric("Mean Fidelity", f"{metrics['mean_fidelity']:.4f}")
        metric_cols[1].metric("Fidelity Std", f"{metrics['std_fidelity']:.4f}")
        metric_cols[2].metric("Mean KL Divergence", f"{metrics['mean_kl']:.4f}")
        metric_cols[3].metric("Recovery Success", 
                             f"{100 * np.mean(np.array(metrics['fidelities']) > 0.9):.1f}%")
        
        # Compare to analytical Petz map
        st.markdown("---")
        st.subheader("ğŸ”„ Comparison: Learned vs Analytical Petz Map")
        
        R_analytical = analytical_petz_map(st.session_state['system'], st.session_state['dt'])
        
        # Test on a specific state
        test_state = st.slider("Test State", 0, system.n_states - 1, 0)
        p0 = np.zeros(system.n_states)
        p0[test_state] = 1.0
        p0 = 0.9 * p0 + 0.1 * system.stationary
        
        p1 = st.session_state['system'].evolve(p0, st.session_state['dt'])
        
        # Analytical recovery
        p_analytical = R_analytical @ p1
        
        # Learned recovery
        device = next(st.session_state['model'].parameters()).device
        p1_tensor = torch.FloatTensor(p1).unsqueeze(0).to(device)
        with torch.no_grad():
            p_learned = st.session_state['model'](p1_tensor).cpu().numpy().squeeze()
        
        fig_compare = go.Figure()
        fig_compare.add_trace(go.Bar(
            x=list(range(min(30, system.n_states))),
            y=p0[:30],
            name='Original',
            marker_color='#00d4ff',
            opacity=0.7
        ))
        fig_compare.add_trace(go.Bar(
            x=list(range(min(30, system.n_states))),
            y=p_learned[:30],
            name='Learned Recovery',
            marker_color='#ff6b6b',
            opacity=0.7
        ))
        fig_compare.add_trace(go.Bar(
            x=list(range(min(30, system.n_states))),
            y=p_analytical[:30],
            name='Analytical Petz',
            marker_color='#4ecdc4',
            opacity=0.7
        ))
        fig_compare.update_layout(
            title="Distribution Comparison (first 30 states)",
            xaxis_title="State",
            yaxis_title="Probability",
            barmode='group',
            height=400
        )
        st.plotly_chart(fig_compare, use_container_width=True)
        
        # Fidelity comparison
        fid_learned = classical_fidelity(p_learned, p0)
        fid_analytical = classical_fidelity(p_analytical, p0)
        
        st.markdown(f"""
        **Recovery Comparison:**
        - Learned Network Fidelity: **{fid_learned:.4f}**
        - Analytical Petz Fidelity: **{fid_analytical:.4f}**
        - Difference: {abs(fid_learned - fid_analytical):.4f}
        """)
    
    # Theory section
    st.markdown("---")
    st.subheader("ğŸ“š The Theory: Why This Works")
    
    st.markdown("""
    ### The Coherence Obstruction
    
    The theorem in `src/SGC/Bridge/CoherenceObstruction.lean` proves that classical 
    dynamics **cannot internally correct drift**. The Knill-Laflamme coefficient Î± 
    must equal zero for any classical embedding.
    
    ### The Petz Solution
    
    The Petz Recovery Map (defined in `src/SGC/Bridge/Recovery.lean`) provides the 
    **external correction**:
    
    1. **Definition**: â„› = adjoint of forward channel w.r.t. weighted inner product
    2. **Property**: Satisfies âŸ¨â„›(Ï), ÏƒâŸ©_Ï€ = âŸ¨Ï, ğ’©(Ïƒ)âŸ©_Ï€
    3. **Classical Form**: Reduces to Bayesian inversion P(x|y) = P(y|x)P(x)/P(y)
    
    ### The MaxEnt Principle
    
    The loss function (Variational Free Energy) implements the **Maximum Entropy** 
    principle:
    
    - Minimize: F = -E[log p(target|pred)] + Î²Â·D(pred||Ï€)
    - This recovers with **minimal assumptions** about lost information
    - The network learns the Petz map as the optimal solution!
    
    ### Landauer's Cost
    
    The neural network "pays" for recovery through computation:
    
    - Measurement of current state (energy cost)
    - Inference to predict previous state (computation = entropy production)
    - This is the thermodynamic cost of information processing (Landauer's principle)
    
    > *The generator L cannot self-correct (Coherence Obstruction), but an external 
    > agent can correct it by paying the thermodynamic cost.*
    """)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI INTERFACE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_cli(args):
    """Command-line training and evaluation."""
    print("=" * 60)
    print("Petz Recovery Network Training")
    print("=" * 60)
    
    if not TORCH_AVAILABLE:
        print("ERROR: PyTorch not installed. Install with: pip install torch")
        return
    
    # Create system
    print(f"\nCreating {args.wells}-well metastable system with {args.dim} states...")
    system = SGCSystem.create_multiwell(args.dim, args.wells)
    print(f"  Inter-well rate: 0.1 (metastability ratio: 10x)")
    
    # Train network
    print(f"\nTraining for {args.epochs} epochs...")
    model, loss_history = train_petz_network(
        system,
        n_trajectories=100,
        trajectory_length=20,
        dt=0.1,
        epochs=args.epochs,
        verbose=True
    )
    
    # Evaluate
    print("\nEvaluating recovery quality...")
    metrics = evaluate_recovery(model, system)
    
    print("\n" + "=" * 60)
    print("RESULTS")
    print("=" * 60)
    print(f"Mean Fidelity:      {metrics['mean_fidelity']:.4f} Â± {metrics['std_fidelity']:.4f}")
    print(f"Mean KL Divergence: {metrics['mean_kl']:.4f} Â± {metrics['std_kl']:.4f}")
    print(f"Recovery Success:   {100 * np.mean(np.array(metrics['fidelities']) > 0.9):.1f}%")
    
    # Compare to analytical
    print("\nComparing to analytical Petz map...")
    R_analytical = analytical_petz_map(system, 0.1)
    
    # Test on random states
    analytical_fidelities = []
    for _ in range(100):
        init_state = np.random.randint(system.n_states)
        p0 = np.zeros(system.n_states)
        p0[init_state] = 1.0
        p0 = 0.9 * p0 + 0.1 * system.stationary
        p1 = system.evolve(p0, 0.1)
        p_analytical = R_analytical @ p1
        analytical_fidelities.append(classical_fidelity(p_analytical, p0))
    
    print(f"Analytical Petz Mean Fidelity: {np.mean(analytical_fidelities):.4f}")
    print(f"Learned Network Mean Fidelity: {metrics['mean_fidelity']:.4f}")
    
    print("\nâœ“ Training complete! The network has learned the Petz Recovery Map.")
    print("  This demonstrates that external ML agents can correct drift that")
    print("  the generator cannot correct internally (Coherence Obstruction).")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import sys
    
    # Check if running via streamlit
    if STREAMLIT_AVAILABLE and 'streamlit' in sys.modules:
        run_streamlit_app()
    else:
        # CLI mode
        parser = argparse.ArgumentParser(
            description="Train a neural network to learn the Petz Recovery Map"
        )
        parser.add_argument('--wells', type=int, default=3, 
                          help='Number of metastable wells')
        parser.add_argument('--dim', type=int, default=50, 
                          help='Total number of states')
        parser.add_argument('--epochs', type=int, default=500, 
                          help='Training epochs')
        parser.add_argument('--streamlit', action='store_true',
                          help='Launch Streamlit app')
        
        args = parser.parse_args()
        
        if args.streamlit:
            if STREAMLIT_AVAILABLE:
                run_streamlit_app()
            else:
                print("Streamlit not installed. Install with: pip install streamlit plotly")
        else:
            run_cli(args)
