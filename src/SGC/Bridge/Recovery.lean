/-
Copyright (c) 2026 SGC Project. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: SGC Formalization Team
-/
import SGC.Bridge.CoherenceObstruction
import SGC.Axioms.WeightedSpace

/-!
# Petz Recovery Map: The External Correction Channel

This module formalizes the **Petz Recovery Map**â€”the canonical recovery channel that
solves the "drift" problem identified in `CoherenceObstruction.lean`.

## Key Insight

The "No Coherent Backaction" theorem proves that a classical system cannot *internally*
correct errors (due to positivity constraints on stochastic matrices). However, it *can*
be corrected by an **external agent** performing Bayesian inversion.

In Quantum Information, this inversion is called the **Petz Recovery Map**.
In Machine Learning, it is **Variational Inference** (or the reverse step in Diffusion Models).

## Mathematical Definition

For a forward channel ğ’© with stationary state Ïƒ, the Petz map â„› is:
  â„›_Ïƒ(Â·) = Ïƒ^{1/2} ğ’©â€ (Ïƒ^{-1/2} (Â·) Ïƒ^{-1/2}) Ïƒ^{1/2}

In our **classical SGC** context (commutative algebra), this simplifies to **Bayesian Inversion**:
  P(x|y) = P(y|x)P(x) / P(y)

## Main Definitions

* `PetzRecoveryMap` - The adjoint operator w.r.t. weighted inner product
* `RelativeEntropy` - KL divergence D(Ïâ€–Ïƒ)
* `DataProcessingInequality` - D(ğ’©Ïâ€–ğ’©Ïƒ) â‰¤ D(Ïâ€–Ïƒ)

## Connection to Machine Learning

The Petz map is the **MaxEnt recovery**: it recovers the state while making the
*fewest* assumptions about lost information. This is why:
- Minimizing Free Energy = Maximizing Entropy of recovery distribution
- Neural networks can *learn* the Petz map via variational inference
- Diffusion models use this for denoising (reverse process)

## References

* [Petz 1986] Sufficient subalgebras and the relative entropy of states
* [Wilde 2013] Quantum Information Theory (Chapter 12)
* [Fawzi-Renner 2015] Quantum conditional mutual information and approximate Markov chains
-/

noncomputable section

namespace SGC.Bridge.Recovery

open SGC.Axioms.GeometryGeneral
open SGC.Axioms.WeightedSpace
open SGC.Bridge.Quantum
open SGC.Bridge.Coherence
open Finset Complex

variable {V : Type*} [Fintype V] [DecidableEq V] [Nonempty V]

/-! ## 1. The Petz Recovery Map

The key insight: In `WeightedSpace`, the **adjoint** w.r.t. the weighted inner product
*is* the Petz recovery map. We expose this as the canonical recovery channel. -/

/-- **Petz Recovery Map**: The adjoint of the forward channel w.r.t. the weighted
    inner product. This is the canonical recovery channel that satisfies:

    âŸ¨â„›(Ï), ÏƒâŸ©_Ï€ = âŸ¨Ï, ğ’©(Ïƒ)âŸ©_Ï€

    For classical (diagonal) states, this reduces to Bayesian inversion:
    P(x|y) = P(y|x)P(x) / P(y) -/
def PetzRecoveryMap (pi_dist : V â†’ â„)
    (forward : (V â†’ â„‚) â†’â‚—[â„‚] (V â†’ â„‚)) : (V â†’ â„‚) â†’â‚—[â„‚] (V â†’ â„‚) :=
  adjoint_pi pi_dist forward

/-- The Petz map satisfies the adjoint property. -/
theorem PetzRecoveryMap_spec (pi_dist : V â†’ â„)
    (forward : (V â†’ â„‚) â†’â‚—[â„‚] (V â†’ â„‚)) (Ï Ïƒ : V â†’ â„‚) :
    SGC.Axioms.GeometryGeneral.inner_pi pi_dist ((PetzRecoveryMap pi_dist forward) Ï) Ïƒ =
    SGC.Axioms.GeometryGeneral.inner_pi pi_dist Ï (forward Ïƒ) :=
  adjoint_pi_spec (ğ•œ := â„‚) pi_dist forward Ï Ïƒ

/-- The Petz map is an involution: â„›(â„›(ğ’©)) = ğ’©. -/
theorem PetzRecoveryMap_involutive (pi_dist : V â†’ â„)
    (forward : (V â†’ â„‚) â†’â‚—[â„‚] (V â†’ â„‚)) :
    PetzRecoveryMap pi_dist (PetzRecoveryMap pi_dist forward) = forward :=
  adjoint_pi_involutive pi_dist forward

/-- Composition rule: â„›(ğ’©â‚ âˆ˜ ğ’©â‚‚) = â„›(ğ’©â‚‚) âˆ˜ â„›(ğ’©â‚). -/
theorem PetzRecoveryMap_comp (pi_dist : V â†’ â„)
    (Nâ‚ Nâ‚‚ : (V â†’ â„‚) â†’â‚—[â„‚] (V â†’ â„‚)) :
    PetzRecoveryMap pi_dist (Nâ‚ âˆ˜â‚— Nâ‚‚) =
    PetzRecoveryMap pi_dist Nâ‚‚ âˆ˜â‚— PetzRecoveryMap pi_dist Nâ‚ :=
  adjoint_pi_comp pi_dist Nâ‚ Nâ‚‚

/-! ## 2. Relative Entropy (KL Divergence)

The relative entropy D(Ïâ€–Ïƒ) measures the "distance" from Ïƒ to Ï. It decreases
under channels (Data Processing Inequality) and is preserved iff the Petz map
perfectly recovers the state. -/

/-- **Relative Entropy** (KL Divergence) for classical distributions.
    D(pâ€–q) = Î£_x p(x) log(p(x)/q(x))

    Convention: 0 log(0/q) = 0, p log(p/0) = +âˆ -/
def RelativeEntropy (p q : V â†’ â„) : â„ :=
  âˆ‘ x, if p x = 0 then 0
       else if q x = 0 then 1000  -- Placeholder for +âˆ
       else p x * Real.log (p x / q x)

/-- Relative entropy is non-negative (Gibbs' inequality). -/
axiom RelativeEntropy_nonneg (p q : V â†’ â„)
    (hp : âˆ€ x, 0 â‰¤ p x) (hq : âˆ€ x, 0 â‰¤ q x)
    (hp_sum : âˆ‘ x, p x = 1) (hq_sum : âˆ‘ x, q x = 1) :
    0 â‰¤ RelativeEntropy p q

/-- D(pâ€–p) = 0. -/
theorem RelativeEntropy_self (p : V â†’ â„) (hp : âˆ€ x, 0 < p x) :
    RelativeEntropy p p = 0 := by
  unfold RelativeEntropy
  apply Finset.sum_eq_zero
  intro x _
  have hpx := hp x
  simp only [ne_of_gt hpx, â†“reduceIte, div_self (ne_of_gt hpx), Real.log_one, mul_zero]

/-- D(pâ€–q) = 0 implies p = q. -/
axiom RelativeEntropy_eq_zero_iff (p q : V â†’ â„)
    (hp : âˆ€ x, 0 < p x) (hq : âˆ€ x, 0 < q x) :
    RelativeEntropy p q = 0 â†” p = q

/-! ## 3. Data Processing Inequality

The fundamental theorem: channels can only destroy information. -/

/-- Apply a stochastic matrix to a distribution. -/
def applyChannel (M : Matrix V V â„) (p : V â†’ â„) : V â†’ â„ :=
  fun y => âˆ‘ x, M y x * p x

/-- **Data Processing Inequality**: Relative entropy decreases under channels.
    D(Mpâ€–Mq) â‰¤ D(pâ€–q)

    This is the information-theoretic version of "you can't get something from nothing."
    Processing data can only lose information, never create it. -/
axiom DataProcessingInequality (M : Matrix V V â„) (p q : V â†’ â„)
    (hM_stoch : âˆ€ y, âˆ‘ x, M y x = 1) (hM_nonneg : âˆ€ y x, 0 â‰¤ M y x)
    (hp : âˆ€ x, 0 â‰¤ p x) (hq : âˆ€ x, 0 â‰¤ q x) :
    RelativeEntropy (applyChannel M p) (applyChannel M q) â‰¤ RelativeEntropy p q

/-- **Petz Recovery Theorem**: Equality in DPI iff Petz map perfectly recovers.

    D(Mpâ€–Mq) = D(pâ€–q)  âŸº  â„›_q(Mp) = p

    This characterizes when information is preserved: exactly when the Petz map
    can perfectly undo the channel's action. -/
axiom PetzRecoveryTheorem (M : Matrix V V â„) (p q : V â†’ â„)
    (hM_stoch : âˆ€ y, âˆ‘ x, M y x = 1) (hM_nonneg : âˆ€ y x, 0 â‰¤ M y x)
    (hp : âˆ€ x, 0 < p x) (hq : âˆ€ x, 0 < q x) :
    RelativeEntropy (applyChannel M p) (applyChannel M q) = RelativeEntropy p q â†”
    âˆƒ (R : Matrix V V â„), applyChannel R (applyChannel M p) = p

/-! ## 4. Approximate Recovery

For approximate lumpability, we get approximate recovery with bounded error. -/

/-- **Fidelity** between distributions (classical version of quantum fidelity).
    F(p,q) = (Î£_x âˆš(p(x)q(x)))Â² -/
def ClassicalFidelity (p q : V â†’ â„) : â„ :=
  (âˆ‘ x, Real.sqrt (p x * q x))^2

/-- Fidelity is symmetric. -/
theorem ClassicalFidelity_symm (p q : V â†’ â„) :
    ClassicalFidelity p q = ClassicalFidelity q p := by
  unfold ClassicalFidelity
  congr 1
  apply Finset.sum_congr rfl
  intro x _
  rw [mul_comm]

/-- **Approximate Recovery Bound**: Recovery fidelity is bounded by entropy loss.

    If D(pâ€–q) - D(Mpâ€–Mq) = Îµ (small entropy loss), then the Petz map achieves
    F(â„›(Mp), p) â‰¥ 1 - Îµ.

    This is the classical version of the Fawzi-Renner bound. -/
axiom ApproximateRecoveryBound (M : Matrix V V â„) (p q : V â†’ â„)
    (hM_stoch : âˆ€ y, âˆ‘ x, M y x = 1) (hM_nonneg : âˆ€ y x, 0 â‰¤ M y x)
    (hp : âˆ€ x, 0 < p x) (hq : âˆ€ x, 0 < q x) :
    let Îµ := RelativeEntropy p q - RelativeEntropy (applyChannel M p) (applyChannel M q)
    âˆƒ (R : Matrix V V â„),
      ClassicalFidelity (applyChannel R (applyChannel M p)) p â‰¥ 1 - 2 * Real.sqrt Îµ

/-! ## 5. Connection to the Coherence Obstruction

The Petz map resolves the paradox from CoherenceObstruction.lean:
- The generator L cannot *internally* correct drift (Î± = 0 forced)
- But an *external* Petz recovery map can correct it

This corresponds to:
- Measurement-feedback control in thermodynamics
- The decoder in error correction
- The score function in variational inference -/

/-- **Recovery Channel** for SGC dynamics: the Petz map of the defect operator.

    This is the "external agent" that can correct the drift that the generator
    cannot correct internally (due to the Coherence Obstruction). -/
def SGCRecoveryChannel (pi_dist : V â†’ â„) (hÏ€ : âˆ€ v, 0 < pi_dist v)
    (L : Matrix V V â„) (P : Partition V) : (V â†’ â„‚) â†’â‚—[â„‚] (V â†’ â„‚) :=
  PetzRecoveryMap pi_dist (complexifyDefect pi_dist hÏ€ L P)

/-- The recovery channel is the adjoint of the defect. -/
theorem SGCRecoveryChannel_eq_adjoint (pi_dist : V â†’ â„) (hÏ€ : âˆ€ v, 0 < pi_dist v)
    (L : Matrix V V â„) (P : Partition V) :
    SGCRecoveryChannel pi_dist hÏ€ L P =
    adjoint_pi pi_dist (complexifyDefect pi_dist hÏ€ L P) := rfl

/-- The recovery-defect composition is self-adjoint (â„›âˆ˜E is Hermitian).
    This means âŸ¨â„›(E(Ïˆ)), Ï†âŸ© = âŸ¨Ïˆ, â„›(E(Ï†))âŸ©. -/
theorem recovery_defect_selfadjoint (pi_dist : V â†’ â„) (hÏ€ : âˆ€ v, 0 < pi_dist v)
    (L : Matrix V V â„) (P : Partition V) :
    IsSelfAdjoint_pi pi_dist
      (SGCRecoveryChannel pi_dist hÏ€ L P âˆ˜â‚— complexifyDefect pi_dist hÏ€ L P) := by
  unfold IsSelfAdjoint_pi SGCRecoveryChannel PetzRecoveryMap
  rw [adjoint_pi_comp, adjoint_pi_involutive]

/-! ## 6. Landauer's Principle: The Cost of Recovery

The Petz recovery is not freeâ€”it requires energy dissipation.
This connects to Landauer's principle: erasing 1 bit costs kT ln(2) energy. -/

/-- **Landauer Cost**: The minimum energy required to implement the recovery map.
    For classical systems, this equals kT times the entropy production. -/
def LandauerCost (pi_dist : V â†’ â„) (kT : â„) (p_initial p_final : V â†’ â„) : â„ :=
  kT * (RelativeEntropy p_final pi_dist - RelativeEntropy p_initial pi_dist)

/-- Landauer's principle: recovery requires positive energy if entropy decreases.
    Î”S < 0 âŸ¹ W â‰¥ kT|Î”S| -/
axiom LandauerPrinciple (pi_dist : V â†’ â„) (kT : â„) (hkT : 0 < kT)
    (p_initial p_final : V â†’ â„) (hp_i : âˆ€ x, 0 < p_initial x) (hp_f : âˆ€ x, 0 < p_final x) :
    LandauerCost pi_dist kT p_initial p_final â‰¥ 0

/-- **The Resolution**: The ML agent (neural network) implements the Petz map
    by "paying" the Landauer cost through computation.

    This formalizes the resolution:
    1. Classical dynamics cannot self-correct (Coherence Obstruction)
    2. External agents can learn correction via measurement-feedback
    3. The thermodynamic cost is exactly Landauer's bound -/
theorem ML_agent_pays_landauer (pi_dist : V â†’ â„) (_hÏ€ : âˆ€ v, 0 < pi_dist v)
    (kT : â„) (hkT : 0 < kT) (_L : Matrix V V â„) (_P : Partition V)
    (p_drift p_corrected : V â†’ â„) (hp_d : âˆ€ x, 0 < p_drift x) (hp_c : âˆ€ x, 0 < p_corrected x) :
    -- The ML agent corrects drift, but must pay energy
    LandauerCost pi_dist kT p_drift p_corrected â‰¥ 0 :=
  LandauerPrinciple pi_dist kT hkT p_drift p_corrected hp_d hp_c

end SGC.Bridge.Recovery
