/-
Copyright (c) 2026 SGC Project. All rights reserved.
Released under Apache 2.0 license as described in the file LICENSE.
Authors: SGC Formalization Team
-/
import SGC.Bridge.CoherenceObstruction
import SGC.Axioms.WeightedSpace
import Mathlib.Analysis.SpecialFunctions.Log.Basic

/-!
# Petz Recovery Map: The External Correction Channel

This module formalizes the **Petz Recovery Map**‚Äîthe canonical recovery channel that
solves the "drift" problem identified in `CoherenceObstruction.lean`.

## Key Insight

The "No Coherent Backaction" theorem proves that a classical system cannot *internally*
correct errors (due to positivity constraints on stochastic matrices). However, it *can*
be corrected by an **external agent** performing Bayesian inversion.

In Quantum Information, this inversion is called the **Petz Recovery Map**.
In Machine Learning, it is **Variational Inference** (or the reverse step in Diffusion Models).

## Mathematical Definition

For a forward channel ùí© with stationary state œÉ, the Petz map ‚Ñõ is:
  ‚Ñõ_œÉ(¬∑) = œÉ^{1/2} ùí©‚Ä†(œÉ^{-1/2} (¬∑) œÉ^{-1/2}) œÉ^{1/2}

In our **classical SGC** context (commutative algebra), this simplifies to **Bayesian Inversion**:
  P(x|y) = P(y|x)P(x) / P(y)

## Main Definitions

* `PetzRecoveryMap` - The adjoint operator w.r.t. weighted inner product
* `RelativeEntropy` - KL divergence D(œÅ‚ÄñœÉ)
* `DataProcessingInequality` - D(ùí©œÅ‚Äñùí©œÉ) ‚â§ D(œÅ‚ÄñœÉ)

## Connection to Machine Learning

The Petz map is the **MaxEnt recovery**: it recovers the state while making the
*fewest* assumptions about lost information. This is why:
- Minimizing Free Energy = Maximizing Entropy of recovery distribution
- Neural networks can *learn* the Petz map via variational inference
- Diffusion models use this for denoising (reverse process)

## References

* [Petz 1986] Sufficient subalgebras and the relative entropy of states
* [Wilde 2013] Quantum Information Theory (Chapter 12)
* [Fawzi-Renner 2015] Quantum conditional mutual information and approximate Markov chains
-/

noncomputable section

namespace SGC.Bridge.Recovery

open SGC.Axioms.GeometryGeneral
open SGC.Axioms.WeightedSpace
open SGC.Bridge.Quantum
open SGC.Bridge.Coherence
open Finset Complex

variable {V : Type*} [Fintype V] [DecidableEq V] [Nonempty V]

/-! ## 1. The Petz Recovery Map

The key insight: In `WeightedSpace`, the **adjoint** w.r.t. the weighted inner product
*is* the Petz recovery map. We expose this as the canonical recovery channel. -/

/-- **Petz Recovery Map**: The adjoint of the forward channel w.r.t. the weighted
    inner product. This is the canonical recovery channel that satisfies:

    ‚ü®‚Ñõ(œÅ), œÉ‚ü©_œÄ = ‚ü®œÅ, ùí©(œÉ)‚ü©_œÄ

    For classical (diagonal) states, this reduces to Bayesian inversion:
    P(x|y) = P(y|x)P(x) / P(y) -/
def PetzRecoveryMap (pi_dist : V ‚Üí ‚Ñù)
    (forward : (V ‚Üí ‚ÑÇ) ‚Üí‚Çó[‚ÑÇ] (V ‚Üí ‚ÑÇ)) : (V ‚Üí ‚ÑÇ) ‚Üí‚Çó[‚ÑÇ] (V ‚Üí ‚ÑÇ) :=
  adjoint_pi pi_dist forward

/-- The Petz map satisfies the adjoint property. -/
theorem PetzRecoveryMap_spec (pi_dist : V ‚Üí ‚Ñù)
    (forward : (V ‚Üí ‚ÑÇ) ‚Üí‚Çó[‚ÑÇ] (V ‚Üí ‚ÑÇ)) (œÅ œÉ : V ‚Üí ‚ÑÇ) :
    SGC.Axioms.GeometryGeneral.inner_pi pi_dist ((PetzRecoveryMap pi_dist forward) œÅ) œÉ =
    SGC.Axioms.GeometryGeneral.inner_pi pi_dist œÅ (forward œÉ) :=
  adjoint_pi_spec (ùïú := ‚ÑÇ) pi_dist forward œÅ œÉ

/-- The Petz map is an involution: ‚Ñõ(‚Ñõ(ùí©)) = ùí©. -/
theorem PetzRecoveryMap_involutive (pi_dist : V ‚Üí ‚Ñù)
    (forward : (V ‚Üí ‚ÑÇ) ‚Üí‚Çó[‚ÑÇ] (V ‚Üí ‚ÑÇ)) :
    PetzRecoveryMap pi_dist (PetzRecoveryMap pi_dist forward) = forward :=
  adjoint_pi_involutive pi_dist forward

/-- Composition rule: ‚Ñõ(ùí©‚ÇÅ ‚àò ùí©‚ÇÇ) = ‚Ñõ(ùí©‚ÇÇ) ‚àò ‚Ñõ(ùí©‚ÇÅ). -/
theorem PetzRecoveryMap_comp (pi_dist : V ‚Üí ‚Ñù)
    (N‚ÇÅ N‚ÇÇ : (V ‚Üí ‚ÑÇ) ‚Üí‚Çó[‚ÑÇ] (V ‚Üí ‚ÑÇ)) :
    PetzRecoveryMap pi_dist (N‚ÇÅ ‚àò‚Çó N‚ÇÇ) =
    PetzRecoveryMap pi_dist N‚ÇÇ ‚àò‚Çó PetzRecoveryMap pi_dist N‚ÇÅ :=
  adjoint_pi_comp pi_dist N‚ÇÅ N‚ÇÇ

/-! ## 2. Relative Entropy (KL Divergence)

The relative entropy D(œÅ‚ÄñœÉ) measures the "distance" from œÉ to œÅ. It decreases
under channels (Data Processing Inequality) and is preserved iff the Petz map
perfectly recovers the state. -/

/-- **Relative Entropy** (KL Divergence) for classical distributions.
    D(p‚Äñq) = Œ£_x p(x) log(p(x)/q(x))

    Convention: 0 log(0/q) = 0, p log(p/0) = +‚àû

    **RIGOROUS VERSION**: Returns `ENNReal` (extended non-negative reals)
    to properly handle the case p(x) > 0 and q(x) = 0 ‚Üí ‚àû. -/
def RelativeEntropy (p q : V ‚Üí ‚Ñù) : ENNReal :=
  ‚àë x, if p x = 0 then 0
       else if q x = 0 then ‚ä§  -- Proper infinity in ENNReal
       else ENNReal.ofReal (p x * Real.log (p x / q x))

/-- Relative entropy is non-negative (trivial for ENNReal). -/
theorem RelativeEntropy_nonneg (p q : V ‚Üí ‚Ñù) : 0 ‚â§ RelativeEntropy p q :=
  zero_le _

/-- D(p‚Äñp) = 0. -/
theorem RelativeEntropy_self (p : V ‚Üí ‚Ñù) (hp : ‚àÄ x, 0 < p x) :
    RelativeEntropy p p = 0 := by
  unfold RelativeEntropy
  apply Finset.sum_eq_zero
  intro x _
  have hpx := hp x
  simp only [ne_of_gt hpx, ‚ÜìreduceIte, div_self (ne_of_gt hpx), Real.log_one, mul_zero,
             ENNReal.ofReal_zero]

/-- D(p‚Äñq) = 0 implies p = q. -/
axiom RelativeEntropy_eq_zero_iff (p q : V ‚Üí ‚Ñù)
    (hp : ‚àÄ x, 0 < p x) (hq : ‚àÄ x, 0 < q x) :
    RelativeEntropy p q = 0 ‚Üî p = q

/-! ## 3. Data Processing Inequality

The fundamental theorem: channels can only destroy information. -/

/-- Apply a stochastic matrix to a distribution. -/
def applyChannel (M : Matrix V V ‚Ñù) (p : V ‚Üí ‚Ñù) : V ‚Üí ‚Ñù :=
  fun y => ‚àë x, M y x * p x

/-- **Data Processing Inequality**: Relative entropy decreases under channels.
    D(Mp‚ÄñMq) ‚â§ D(p‚Äñq)

    This is the information-theoretic version of "you can't get something from nothing."
    Processing data can only lose information, never create it. -/
axiom DataProcessingInequality (M : Matrix V V ‚Ñù) (p q : V ‚Üí ‚Ñù)
    (hM_stoch : ‚àÄ y, ‚àë x, M y x = 1) (hM_nonneg : ‚àÄ y x, 0 ‚â§ M y x)
    (hp : ‚àÄ x, 0 ‚â§ p x) (hq : ‚àÄ x, 0 ‚â§ q x) :
    RelativeEntropy (applyChannel M p) (applyChannel M q) ‚â§ RelativeEntropy p q

/-- **Petz Recovery Theorem**: Equality in DPI iff Petz map perfectly recovers.

    D(Mp‚ÄñMq) = D(p‚Äñq)  ‚ü∫  ‚Ñõ_q(Mp) = p

    This characterizes when information is preserved: exactly when the Petz map
    can perfectly undo the channel's action. -/
axiom PetzRecoveryTheorem (M : Matrix V V ‚Ñù) (p q : V ‚Üí ‚Ñù)
    (hM_stoch : ‚àÄ y, ‚àë x, M y x = 1) (hM_nonneg : ‚àÄ y x, 0 ‚â§ M y x)
    (hp : ‚àÄ x, 0 < p x) (hq : ‚àÄ x, 0 < q x) :
    RelativeEntropy (applyChannel M p) (applyChannel M q) = RelativeEntropy p q ‚Üî
    ‚àÉ (R : Matrix V V ‚Ñù), applyChannel R (applyChannel M p) = p

/-! ## 4. Approximate Recovery

For approximate lumpability, we get approximate recovery with bounded error. -/

/-- **Fidelity** between distributions (classical version of quantum fidelity).
    F(p,q) = (Œ£_x ‚àö(p(x)q(x)))¬≤ -/
def ClassicalFidelity (p q : V ‚Üí ‚Ñù) : ‚Ñù :=
  (‚àë x, Real.sqrt (p x * q x))^2

/-- Fidelity is symmetric. -/
theorem ClassicalFidelity_symm (p q : V ‚Üí ‚Ñù) :
    ClassicalFidelity p q = ClassicalFidelity q p := by
  unfold ClassicalFidelity
  congr 1
  apply Finset.sum_congr rfl
  intro x _
  rw [mul_comm]

/-- **Approximate Recovery Bound**: Recovery fidelity is bounded by entropy loss.

    If D(p‚Äñq) - D(Mp‚ÄñMq) = Œµ (small entropy loss), then the Petz map achieves
    F(‚Ñõ(Mp), p) ‚â• 1 - Œµ.

    This is the classical version of the Fawzi-Renner bound.

    Note: Uses `ENNReal.toReal` for the bound since Œµ is finite when supports are compatible. -/
axiom ApproximateRecoveryBound (M : Matrix V V ‚Ñù) (p q : V ‚Üí ‚Ñù)
    (hM_stoch : ‚àÄ y, ‚àë x, M y x = 1) (hM_nonneg : ‚àÄ y x, 0 ‚â§ M y x)
    (hp : ‚àÄ x, 0 < p x) (hq : ‚àÄ x, 0 < q x) :
    let Œµ := (RelativeEntropy p q - RelativeEntropy (applyChannel M p) (applyChannel M q)).toReal
    ‚àÉ (R : Matrix V V ‚Ñù),
      ClassicalFidelity (applyChannel R (applyChannel M p)) p ‚â• 1 - 2 * Real.sqrt Œµ

/-! ## 5. Connection to the Coherence Obstruction

The Petz map resolves the paradox from CoherenceObstruction.lean:
- The generator L cannot *internally* correct drift (Œ± = 0 forced)
- But an *external* Petz recovery map can correct it

This corresponds to:
- Measurement-feedback control in thermodynamics
- The decoder in error correction
- The score function in variational inference -/

/-- **Recovery Channel** for SGC dynamics: the Petz map of the defect operator.

    This is the "external agent" that can correct the drift that the generator
    cannot correct internally (due to the Coherence Obstruction). -/
def SGCRecoveryChannel (pi_dist : V ‚Üí ‚Ñù) (hœÄ : ‚àÄ v, 0 < pi_dist v)
    (L : Matrix V V ‚Ñù) (P : Partition V) : (V ‚Üí ‚ÑÇ) ‚Üí‚Çó[‚ÑÇ] (V ‚Üí ‚ÑÇ) :=
  PetzRecoveryMap pi_dist (complexifyDefect pi_dist hœÄ L P)

/-- The recovery channel is the adjoint of the defect. -/
theorem SGCRecoveryChannel_eq_adjoint (pi_dist : V ‚Üí ‚Ñù) (hœÄ : ‚àÄ v, 0 < pi_dist v)
    (L : Matrix V V ‚Ñù) (P : Partition V) :
    SGCRecoveryChannel pi_dist hœÄ L P =
    adjoint_pi pi_dist (complexifyDefect pi_dist hœÄ L P) := rfl

/-- The recovery-defect composition is self-adjoint (‚Ñõ‚àòE is Hermitian).
    This means ‚ü®‚Ñõ(E(œà)), œÜ‚ü© = ‚ü®œà, ‚Ñõ(E(œÜ))‚ü©. -/
theorem recovery_defect_selfadjoint (pi_dist : V ‚Üí ‚Ñù) (hœÄ : ‚àÄ v, 0 < pi_dist v)
    (L : Matrix V V ‚Ñù) (P : Partition V) :
    IsSelfAdjoint_pi pi_dist
      (SGCRecoveryChannel pi_dist hœÄ L P ‚àò‚Çó complexifyDefect pi_dist hœÄ L P) := by
  unfold IsSelfAdjoint_pi SGCRecoveryChannel PetzRecoveryMap
  rw [adjoint_pi_comp, adjoint_pi_involutive]

/-! ## 6. Landauer's Principle: The Cost of Recovery

The Petz recovery is not free‚Äîit requires energy dissipation.
This connects to Landauer's principle: erasing 1 bit costs kT ln(2) energy. -/

/-- **Landauer Cost**: The minimum energy required to implement the recovery map.
    For classical systems, this equals kT times the entropy production.

    Note: Uses `ENNReal.toReal` since we assume finite entropy (compatible supports). -/
def LandauerCost (pi_dist : V ‚Üí ‚Ñù) (kT : ‚Ñù) (p_initial p_final : V ‚Üí ‚Ñù) : ‚Ñù :=
  kT * ((RelativeEntropy p_final pi_dist).toReal - (RelativeEntropy p_initial pi_dist).toReal)

/-- Landauer's principle: recovery requires positive energy if entropy decreases.
    ŒîS < 0 ‚üπ W ‚â• kT|ŒîS| -/
axiom LandauerPrinciple (pi_dist : V ‚Üí ‚Ñù) (kT : ‚Ñù) (hkT : 0 < kT)
    (p_initial p_final : V ‚Üí ‚Ñù) (hp_i : ‚àÄ x, 0 < p_initial x) (hp_f : ‚àÄ x, 0 < p_final x) :
    LandauerCost pi_dist kT p_initial p_final ‚â• 0

/-- **The Resolution**: The ML agent (neural network) implements the Petz map
    by "paying" the Landauer cost through computation.

    This formalizes the resolution:
    1. Classical dynamics cannot self-correct (Coherence Obstruction)
    2. External agents can learn correction via measurement-feedback
    3. The thermodynamic cost is exactly Landauer's bound -/
theorem ML_agent_pays_landauer (pi_dist : V ‚Üí ‚Ñù) (_hœÄ : ‚àÄ v, 0 < pi_dist v)
    (kT : ‚Ñù) (hkT : 0 < kT) (_L : Matrix V V ‚Ñù) (_P : Partition V)
    (p_drift p_corrected : V ‚Üí ‚Ñù) (hp_d : ‚àÄ x, 0 < p_drift x) (hp_c : ‚àÄ x, 0 < p_corrected x) :
    -- The ML agent corrects drift, but must pay energy
    LandauerCost pi_dist kT p_drift p_corrected ‚â• 0 :=
  LandauerPrinciple pi_dist kT hkT p_drift p_corrected hp_d hp_c

end SGC.Bridge.Recovery
